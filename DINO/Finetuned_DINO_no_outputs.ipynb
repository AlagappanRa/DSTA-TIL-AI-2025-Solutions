{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBAHUDk4J2NH",
    "outputId": "dbd9e38f-f49c-42e1-a087-bdd1334a5acc"
   },
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "LE203bUeKWuU",
    "outputId": "22c4ccf5-c628-4b43-a8ac-1f6e73cd5d38"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rPvfD9qKiXj",
    "outputId": "aca42f01-1002-4367-f121-f038e1c95e1b"
   },
   "outputs": [],
   "source": [
    "# 3. Configure and secure the token\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wApZmt4MLE5e",
    "outputId": "05fa625b-40a1-4e47-8c30-236c3338e24c"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download alagappanramanathan3/til-cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4r2AhjHNbeh",
    "outputId": "f3abcb4d-89a4-44b7-8d5a-894832f33af2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  til-cv.zip\n",
      "caution: filename not matched:  /til-cv\n"
     ]
    }
   ],
   "source": [
    "!unzip til-cv.zip /til-cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80KzpuFdL62U",
    "outputId": "9c0f6b54-f74f-43c9-d9e6-28cc51d05095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "!ls ./cv/images -1 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvKSd-v2NY8b",
    "outputId": "2fdde0b8-13c7-4b74-939a-a39e00c72998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat './cv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# 6. Copy the dataset folder into Drive\n",
    "!cp -r ./cv /content/drive/MyDrive/cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxXv3YUIMaW5",
    "outputId": "84706405-07ea-4bf1-d56d-fdae063b6551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# 5. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5b2hhmWHSTS",
    "outputId": "ce9ce4f6-0e28-4ea8-eff0-36f6176125be"
   },
   "outputs": [],
   "source": [
    "# Copy the entire cv folder to local disk\n",
    "!rsync -av --ignore-existing -r /content/drive/MyDrive/cv /content/cv\n",
    "\n",
    "# List contents to verify the copy worked\n",
    "!ls -la /content/cv\n",
    "\n",
    "# Check disk usage\n",
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1e2-QZ7brom",
    "outputId": "3e5686be-a2ca-4cab-c843-7a2d79d5295d"
   },
   "outputs": [],
   "source": [
    "# 2. Install PyTorch for A100\n",
    "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNrDB0N9bl-z",
    "outputId": "89a21ffd-3630-49ec-b88d-ebd515b1a4d0"
   },
   "outputs": [],
   "source": [
    "# Downgrade NumPy to 1.26 (latest 1.x version)\n",
    "!pip install \"numpy<2\" --force-reinstall\n",
    "\n",
    "# Or specifically install 1.26.3\n",
    "!pip install numpy==1.26.3 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqGTLDgKY0ip",
    "outputId": "1817a833-b44a-4341-e3b2-fa77fd74a104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DINO'...\n",
      "remote: Enumerating objects: 442, done.\u001b[K\n",
      "remote: Counting objects: 100% (192/192), done.\u001b[K\n",
      "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
      "remote: Total 442 (delta 137), reused 94 (delta 94), pack-reused 250 (from 1)\u001b[K\n",
      "Receiving objects: 100% (442/442), 13.43 MiB | 27.62 MiB/s, done.\n",
      "Resolving deltas: 100% (191/191), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/IDEA-Research/DINO.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDoaWUHScA8N",
    "outputId": "388ccba9-cd82-438a-cb44-eb8e5246e9dd"
   },
   "outputs": [],
   "source": [
    "# 3. Install other requirements\n",
    "!pip install -r DINO/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rp0p3IjuccVS",
    "outputId": "ad76e17b-c208-40b6-ced4-5fe7823ce35f"
   },
   "outputs": [],
   "source": [
    "# Uninstall current PyTorch\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "# Install PyTorch with CUDA 12.1 (closest stable version to 12.5)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_gCG1vjbxeR",
    "outputId": "e8d2aaeb-0a3e-476c-e9df-64d540c4cf0a"
   },
   "outputs": [],
   "source": [
    "# 4. Build custom CUDA ops\n",
    "%cd DINO/models/dino/ops\n",
    "!python setup.py build install\n",
    "%cd /content\n",
    "\n",
    "# Test import\n",
    "!python -c \"import MultiScaleDeformableAttention as MSDA; print('MSDA module loaded successfully')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gdtV1gxM21b",
    "outputId": "b2b81f6a-3ba7-4134-9ad8-af2aea8b82d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "DINO imported successfully\n"
     ]
    }
   ],
   "source": [
    "# 5. Test import\n",
    "!python -c \"import sys; sys.path.append('/content/DINO'); from models.dino.dino import build_dino; print('DINO imported successfully')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5sXoG8z6bTQS",
    "outputId": "4e06b866-b56a-4cbd-b3c2-d9425b1b6e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINO imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import torch BEFORE importing any PyTorch extensions\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Now test the DINO import\n",
    "import sys\n",
    "sys.path.append('/content/DINO')\n",
    "from models.dino.dino import build_dino\n",
    "print('DINO imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZLSMyoadvnO",
    "outputId": "79cc237d-b2a6-48af-a5a1-ca0060bd0023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/DINO/models/dino/ops/build/lib.linux-x86_64-cpython-311/MultiScaleDeformableAttention.cpython-311-x86_64-linux-gnu.so\n"
     ]
    }
   ],
   "source": [
    "# Check if the shared library was created\n",
    "!find /content/DINO -name \"*.so\" | grep -i multiscale\n",
    "!find ~/.cache/torch_extensions -name \"*.so\" 2>/dev/nul l | grep -i multiscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45hbU37leAj8",
    "outputId": "67377bdf-b36a-4c99-8376-251f81797810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/DINO/models/dino/ops\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd /content/DINO/models/dino/ops\n",
    "\n",
    "# Copy the built module to the correct location\n",
    "!cp build/lib.linux-x86_64-cpython-311/MultiScaleDeformableAttention.cpython-311-x86_64-linux-gnu.so ./\n",
    "\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltoAOXDAecdy",
    "outputId": "786cef76-b176-4f19-b96e-ae87ef8cbbe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSDA loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/content/DINO/models/dino/ops/build/lib.linux-x86_64-cpython-311')\n",
    "\n",
    "# Now test import\n",
    "import MultiScaleDeformableAttention as MSDA\n",
    "print('MSDA loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzJY-Qz-eis9",
    "outputId": "aa6dd4ec-c490-477f-b35a-537c84de2c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSDA loaded successfully\n",
      "DINO imported successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('/content/DINO')\n",
    "\n",
    "# Test the custom ops first\n",
    "import MultiScaleDeformableAttention as MSDA\n",
    "print('MSDA loaded successfully')\n",
    "\n",
    "# Now test DINO\n",
    "from models.dino.dino import build_dino\n",
    "print('DINO imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0k9rAqKkwXC"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "BASE_PATH = Path(\"/content/drive/MyDrive/cv/cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDh5c-rNek9-",
    "outputId": "5ab53b34-947d-4017-e582-f953a6dc79f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANNOTATIONS.JSON STRUCTURE ANALYSIS ===\n",
      "Type of root object: <class 'dict'>\n",
      "Top-level keys: ['info', 'images', 'annotations', 'licenses', 'categories']\n",
      "\n",
      "--- Key: 'info' ---\n",
      "Type: <class 'dict'>\n",
      "Dict keys: []\n",
      "\n",
      "--- Key: 'images' ---\n",
      "Type: <class 'list'>\n",
      "Length: 20000\n",
      "First item type: <class 'dict'>\n",
      "First item sample: {'id': 9146, 'width': 1920, 'height': 1080, 'file_name': '9146.jpg'}\n",
      "\n",
      "--- Key: 'annotations' ---\n",
      "Type: <class 'list'>\n",
      "Length: 72967\n",
      "First item type: <class 'dict'>\n",
      "First item sample: {'id': 0, 'image_id': 9146, 'category_id': 1, 'area': 3861.0041370397084, 'bbox': [2.9999542236328125, 315.9999918937683, 99.00009155273438, 39.0000057220459], 'iscrowd': 0}\n",
      "\n",
      "--- Key: 'licenses' ---\n",
      "Type: <class 'list'>\n",
      "Length: 0\n",
      "\n",
      "--- Key: 'categories' ---\n",
      "Type: <class 'list'>\n",
      "Length: 18\n",
      "First item type: <class 'dict'>\n",
      "First item sample: {'id': 0, 'name': 'cargo aircraft'}\n",
      "\n",
      "Detected format: COCO format\n",
      "\n",
      "=== IMAGE FILES VERIFICATION ===\n",
      "Number of .jpg files in images folder: 20000\n",
      "Sample image filenames: ['9099.jpg', '91.jpg', '910.jpg', '9100.jpg', '9101.jpg', '9102.jpg', '9103.jpg', '9104.jpg', '9105.jpg', '9106.jpg']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Load and analyze annotations.json structure\n",
    "with open(BASE_PATH / \"annotations.json\", 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "print(\"=== ANNOTATIONS.JSON STRUCTURE ANALYSIS ===\")\n",
    "print(f\"Type of root object: {type(annotations)}\")\n",
    "\n",
    "if isinstance(annotations, dict):\n",
    "    print(f\"Top-level keys: {list(annotations.keys())}\")\n",
    "\n",
    "    # Analyze each top-level key\n",
    "    for key in annotations.keys():\n",
    "        print(f\"\\n--- Key: '{key}' ---\")\n",
    "        print(f\"Type: {type(annotations[key])}\")\n",
    "\n",
    "        if isinstance(annotations[key], list):\n",
    "            print(f\"Length: {len(annotations[key])}\")\n",
    "            if len(annotations[key]) > 0:\n",
    "                print(f\"First item type: {type(annotations[key][0])}\")\n",
    "                print(f\"First item sample: {annotations[key][0]}\")\n",
    "\n",
    "        elif isinstance(annotations[key], dict):\n",
    "            print(f\"Dict keys: {list(annotations[key].keys())[:10]}\")  # First 10 keys\n",
    "\n",
    "elif isinstance(annotations, list):\n",
    "    print(f\"List length: {len(annotations)}\")\n",
    "    if len(annotations) > 0:\n",
    "        print(f\"First item: {annotations[0]}\")\n",
    "\n",
    "# Check if it follows any known annotation format patterns\n",
    "def check_annotation_format(data):\n",
    "    if isinstance(data, dict):\n",
    "        # Check for COCO format\n",
    "        if all(key in data for key in ['images', 'annotations', 'categories']):\n",
    "            return \"COCO format\"\n",
    "        # Check for YOLO format indicators\n",
    "        elif 'labels' in data or 'bboxes' in data:\n",
    "            return \"Possible YOLO format\"\n",
    "    return \"Custom format\"\n",
    "\n",
    "format_type = check_annotation_format(annotations)\n",
    "print(f\"\\nDetected format: {format_type}\")\n",
    "\n",
    "# Verify image files\n",
    "print(f\"\\n=== IMAGE FILES VERIFICATION ===\")\n",
    "image_count = len([f for f in os.listdir(BASE_PATH / \"images\") if f.endswith('.jpg')])\n",
    "print(f\"Number of .jpg files in images folder: {image_count}\")\n",
    "\n",
    "# Check image naming pattern\n",
    "image_files = [f for f in os.listdir(BASE_PATH / \"images\") if f.endswith('.jpg')][:10]\n",
    "print(f\"Sample image filenames: {image_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxMdUZm2g4lY",
    "outputId": "96933d41-133d-4982-a1fd-8a74727ef7f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CATEGORIES ANALYSIS ===\n",
      "ID: 0, Name: cargo aircraft\n",
      "ID: 1, Name: commercial aircraft\n",
      "ID: 2, Name: drone\n",
      "ID: 3, Name: fighter jet\n",
      "ID: 4, Name: fighter plane\n",
      "ID: 5, Name: helicopter\n",
      "ID: 6, Name: light aircraft\n",
      "ID: 7, Name: missile\n",
      "ID: 8, Name: truck\n",
      "ID: 9, Name: car\n",
      "ID: 10, Name: tank\n",
      "ID: 11, Name: bus\n",
      "ID: 12, Name: van\n",
      "ID: 13, Name: cargo ship\n",
      "ID: 14, Name: yacht\n",
      "ID: 15, Name: cruise ship\n",
      "ID: 16, Name: warship\n",
      "ID: 17, Name: sailboat\n",
      "\n",
      "=== DATA STATISTICS ===\n",
      "Total images: 20000\n",
      "Total annotations: 72967\n",
      "\n",
      "Annotations per category:\n",
      "  0: cargo aircraft (1189 annotations)\n",
      "  1: commercial aircraft (6769 annotations)\n",
      "  2: drone (3865 annotations)\n",
      "  3: fighter jet (9792 annotations)\n",
      "  4: fighter plane (4906 annotations)\n",
      "  5: helicopter (6299 annotations)\n",
      "  6: light aircraft (3663 annotations)\n",
      "  7: missile (3200 annotations)\n",
      "  8: truck (8456 annotations)\n",
      "  9: car (3395 annotations)\n",
      "  10: tank (8650 annotations)\n",
      "  11: bus (4897 annotations)\n",
      "  12: van (2502 annotations)\n",
      "  13: cargo ship (1270 annotations)\n",
      "  14: yacht (1255 annotations)\n",
      "  15: cruise ship (804 annotations)\n",
      "  16: warship (1041 annotations)\n",
      "  17: sailboat (1014 annotations)\n",
      "\n",
      "✓ All annotation image_ids have corresponding images\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the annotations\n",
    "with open(BASE_PATH / 'annotations.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"=== CATEGORIES ANALYSIS ===\")\n",
    "categories = data['categories']\n",
    "for cat in categories:\n",
    "    print(f\"ID: {cat['id']}, Name: {cat['name']}\")\n",
    "\n",
    "print(f\"\\n=== DATA STATISTICS ===\")\n",
    "print(f\"Total images: {len(data['images'])}\")\n",
    "print(f\"Total annotations: {len(data['annotations'])}\")\n",
    "\n",
    "# Check category ID distribution\n",
    "from collections import Counter\n",
    "cat_counts = Counter([ann['category_id'] for ann in data['annotations']])\n",
    "print(f\"\\nAnnotations per category:\")\n",
    "for cat_id, count in sorted(cat_counts.items()):\n",
    "    cat_name = next((cat['name'] for cat in categories if cat['id'] == cat_id), 'Unknown')\n",
    "    print(f\"  {cat_id}: {cat_name} ({count} annotations)\")\n",
    "\n",
    "# Check for missing images\n",
    "image_ids = set(img['id'] for img in data['images'])\n",
    "annotation_image_ids = set(ann['image_id'] for ann in data['annotations'])\n",
    "missing_images = annotation_image_ids - image_ids\n",
    "if missing_images:\n",
    "    print(f\"\\nWARNING: {len(missing_images)} annotations reference missing images\")\n",
    "else:\n",
    "    print(f\"\\n✓ All annotation image_ids have corresponding images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cVgLgLH9iLZV",
    "outputId": "6f73e860-cfce-43d1-cc52-f50ef5bad99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using original category IDs (expected to be 0-based).\n",
      "✓ Train split: 16000 images, 58057 annotations\n",
      "✓ Val split: 4000 images, 14910 annotations\n",
      "✓ Saved to coco_dataset/annotations/\n",
      "\n",
      "Category ID mapping (no change applied): {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "!rm -rf coco_dataset/\n",
    "\n",
    "def convert_coco_split(input_file, output_dir, train_split=0.8): # Renamed for clarity\n",
    "    \"\"\"\n",
    "    Splits a COCO format annotation file into training and validation sets.\n",
    "    Category IDs are preserved (expected to be 0-based).\n",
    "    \"\"\"\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # --- Removed category ID shifting ---\n",
    "    # The original category IDs from data['categories'] will be used directly.\n",
    "    # Annotations in data['annotations'] should already use these original IDs.\n",
    "    print(\"Using original category IDs (expected to be 0-based).\")\n",
    "\n",
    "    # Create a mapping of original IDs to themselves for clarity,\n",
    "    # or if any downstream process still expects a 'category_mapping'.\n",
    "    category_mapping = {cat['id']: cat['id'] for cat in data['categories']}\n",
    "\n",
    "    # --- End of removed/modified section ---\n",
    "\n",
    "    # Create train/val split\n",
    "    images = data['images']\n",
    "    train_images, val_images = train_test_split(images, train_size=train_split, random_state=42)\n",
    "\n",
    "    train_image_ids = set(img['id'] for img in train_images)\n",
    "    val_image_ids = set(img['id'] for img in val_images)\n",
    "\n",
    "    # Split annotations\n",
    "    # No changes needed here as 'category_id' in annotations should already match the original 'id' in categories\n",
    "    train_annotations = [ann for ann in data['annotations'] if ann['image_id'] in train_image_ids]\n",
    "    val_annotations = [ann for ann in data['annotations'] if ann['image_id'] in val_image_ids]\n",
    "\n",
    "    # Create train dataset\n",
    "    train_data = {\n",
    "        'info': data['info'],\n",
    "        'licenses': data['licenses'],\n",
    "        'categories': data['categories'], # Categories are used as is\n",
    "        'images': train_images,\n",
    "        'annotations': train_annotations # Annotations are used as is\n",
    "    }\n",
    "\n",
    "    # Create val dataset\n",
    "    val_data = {\n",
    "        'info': data['info'],\n",
    "        'licenses': data['licenses'],\n",
    "        'categories': data['categories'], # Categories are used as is\n",
    "        'images': val_images,\n",
    "        'annotations': val_annotations # Annotations are used as is\n",
    "    }\n",
    "\n",
    "    # Create output directory structure\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/annotations\", exist_ok=True)\n",
    "\n",
    "    # Save datasets\n",
    "    with open(f\"{output_dir}/annotations/instances_train2017.json\", 'w') as f:\n",
    "        json.dump(train_data, f)\n",
    "\n",
    "    with open(f\"{output_dir}/annotations/instances_val2017.json\", 'w') as f:\n",
    "        json.dump(val_data, f)\n",
    "\n",
    "    print(f\"✓ Train split: {len(train_images)} images, {len(train_annotations)} annotations\")\n",
    "    print(f\"✓ Val split: {len(val_images)} images, {len(val_annotations)} annotations\")\n",
    "    print(f\"✓ Saved to {output_dir}/annotations/\")\n",
    "\n",
    "    return category_mapping\n",
    "\n",
    "# Convert your data using the modified function\n",
    "category_mapping = convert_coco_split(BASE_PATH / 'annotations.json', 'coco_dataset')\n",
    "print(f\"\\nCategory ID mapping (no change applied): {category_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "d74b69b23a7249b69cd0add89f458f6c",
      "87a81102cf2a43ee99ec093af83d0560",
      "04a88b242f954f97a0b9d52ee8a11a49",
      "cac14aeac8b54383aea0f73377642f1b",
      "a3c5c88ac00347b994f066e40d56c54e",
      "4e97eb37c2444e67b77c7cabb680f8de",
      "ef04f49b5eff470a86b96c814ede3208",
      "93d6ebf54dea460fa2b473010a5161b1",
      "002cf07e6a84494aa7c1999fbc64309c",
      "b4f9d18e168e4e75a69c7b03f12f3ce9",
      "de20d30f83b24b67845eb6581b3208d1"
     ]
    },
    "id": "focoK6c8iMcW",
    "outputId": "5fe3c213-16fa-4ecc-ab53-49c9d48369b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 16000\n",
      "Val images: 4000\n",
      "Found 20000 total image files\n",
      "Copying 16000 train images and 4000 val images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74b69b23a7249b69cd0add89f458f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copying images to train/val splits:   0%|          | 0/20000 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Copied 16000 images to train2017/\n",
      "✓ Copied 4000 images to val2017/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def copy_file(src_dst):\n",
    "    \"\"\"Copy a single file - function for threading\"\"\"\n",
    "    src_path, dst_path = src_dst\n",
    "    # Ensure destination directory exists\n",
    "    dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "    return dst_path\n",
    "\n",
    "def setup_tao_directory_with_split(base_path=None, annotations_dir='coco_dataset/annotations'):\n",
    "    \"\"\"Copy images according to the train/val split in annotations\"\"\"\n",
    "\n",
    "    if base_path is None:\n",
    "        base_path = BASE_PATH\n",
    "        if base_path is None:\n",
    "            return False\n",
    "\n",
    "    base_path = Path(base_path)\n",
    "    images_dir = base_path / 'images'\n",
    "\n",
    "    if not images_dir.exists():\n",
    "        print(f\"Images directory not found at {images_dir}\")\n",
    "        return False\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs('coco_dataset/train2017', exist_ok=True)\n",
    "    os.makedirs('coco_dataset/val2017', exist_ok=True)\n",
    "\n",
    "    # Read the annotation files to get the correct image splits\n",
    "    train_annotations_file = f\"{annotations_dir}/instances_train2017.json\"\n",
    "    val_annotations_file = f\"{annotations_dir}/instances_val2017.json\"\n",
    "\n",
    "    if not os.path.exists(train_annotations_file) or not os.path.exists(val_annotations_file):\n",
    "        print(\"Error: Annotation files not found. Run convert_to_tao_format first.\")\n",
    "        return False\n",
    "\n",
    "    # Load annotation files to get image lists\n",
    "    with open(train_annotations_file, 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "\n",
    "    with open(val_annotations_file, 'r') as f:\n",
    "        val_data = json.load(f)\n",
    "\n",
    "    # Extract image filenames from annotations\n",
    "    train_images = {img['file_name'] for img in train_data['images']}\n",
    "    val_images = {img['file_name'] for img in val_data['images']}\n",
    "\n",
    "    print(f\"Train images: {len(train_images)}\")\n",
    "    print(f\"Val images: {len(val_images)}\")\n",
    "\n",
    "    # Get all image files\n",
    "    all_image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
    "    print(f\"Found {len(all_image_files)} total image files\")\n",
    "\n",
    "    # Create copy tasks for train and val separately\n",
    "    train_tasks = []\n",
    "    val_tasks = []\n",
    "\n",
    "    for img_file in all_image_files:\n",
    "        src_path = images_dir / img_file\n",
    "        if img_file in train_images:\n",
    "            dst_path = Path('coco_dataset/train2017') / img_file\n",
    "            train_tasks.append((src_path, dst_path))\n",
    "        elif img_file in val_images:\n",
    "            dst_path = Path('coco_dataset/val2017') / img_file\n",
    "            val_tasks.append((src_path, dst_path))\n",
    "\n",
    "    all_tasks = train_tasks + val_tasks\n",
    "    print(f\"Copying {len(train_tasks)} train images and {len(val_tasks)} val images...\")\n",
    "\n",
    "    # Use ThreadPoolExecutor for I/O-bound tasks\n",
    "    max_workers = min(32, (os.cpu_count() or 1) + 4)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        list(tqdm(\n",
    "            executor.map(copy_file, all_tasks),\n",
    "            total=len(all_tasks),\n",
    "            desc='Copying images to train/val splits',\n",
    "            unit='files'\n",
    "        ))\n",
    "\n",
    "    print(f\"✓ Copied {len(train_tasks)} images to train2017/\")\n",
    "    print(f\"✓ Copied {len(val_tasks)} images to val2017/\")\n",
    "\n",
    "    return True\n",
    "\n",
    "# Run the corrected setup\n",
    "setup_tao_directory_with_split(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yltfXJHj1gqw",
    "outputId": "7e29f5b7-e50e-460e-ea26-ef53c95fc9e2"
   },
   "outputs": [],
   "source": [
    "!pip install -U gdown\n",
    "#!gdown --id 1CrzFP0RycSC24KKmF5k0libLRJgpX9x0 --output checkpoint0029_4scale_swin.pth\n",
    "!gdown --id 14h4UCi-HsDL01ZQRbpV47dzMST_py_vM --output checkpoint0029_5scale_swin.pth\n",
    "#https://drive.google.com/file/d/14h4UCi-HsDL01ZQRbpV47dzMST_py_vM/view?usp=drive_link\n",
    "#https://drive.google.com/file/d/1CrzFP0RycSC24KKmF5k0libLRJgpX9x0/view?usp=drive_link\n",
    "#https://drive.google.com/file/d/14h4UCi-HsDL01ZQRbpV47dzMST_py_vM/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8evJnQXMmJn8"
   },
   "outputs": [],
   "source": [
    "!mkdir configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "groEahcaDSR1"
   },
   "source": [
    "# Swin 5 scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OxR2G-iDR-Q"
   },
   "outputs": [],
   "source": [
    "config_content = \"\"\"data_aug_scales = [400, 480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800, 832]\n",
    "data_aug_max_size = 1333\n",
    "data_aug_scales2_resize = [400, 500, 600]\n",
    "data_aug_scales2_crop = [384, 600]\n",
    "\n",
    "data_aug_scale_overlap = None\n",
    "\n",
    "num_classes=18\n",
    "dn_labelbook_size=19\n",
    "\n",
    "lr = 0.0001\n",
    "param_dict_type = 'default'\n",
    "lr_backbone = 1e-05\n",
    "lr_backbone_names = ['backbone.0']\n",
    "lr_linear_proj_names = ['reference_points', 'sampling_offsets']\n",
    "lr_linear_proj_mult = 0.1\n",
    "ddetr_lr_param = False\n",
    "batch_size = 2                    # Reduced for 5-scale\n",
    "weight_decay = 0.0001\n",
    "epochs = 12\n",
    "lr_drop = 11\n",
    "save_checkpoint_interval = 1\n",
    "clip_max_norm = 0.1\n",
    "onecyclelr = False\n",
    "multi_step_lr = False\n",
    "lr_drop_list = [33, 45]\n",
    "\n",
    "modelname = 'dino'\n",
    "frozen_weights = None\n",
    "backbone = 'swin_L_384_22k'\n",
    "use_checkpoint = True\n",
    "\n",
    "dilation = False\n",
    "position_embedding = 'sine'\n",
    "pe_temperatureH = 20\n",
    "pe_temperatureW = 20\n",
    "return_interm_indices = [0, 1, 2, 3]    # 5-scale indices\n",
    "backbone_freeze_keywords = None\n",
    "enc_layers = 6\n",
    "dec_layers = 6\n",
    "unic_layers = 0\n",
    "pre_norm = False\n",
    "dim_feedforward = 2048\n",
    "hidden_dim = 256\n",
    "dropout = 0.0\n",
    "nheads = 8\n",
    "num_queries = 900\n",
    "query_dim = 4\n",
    "num_patterns = 0\n",
    "pdetr3_bbox_embed_diff_each_layer = False\n",
    "pdetr3_refHW = -1\n",
    "random_refpoints_xy = False\n",
    "fix_refpoints_hw = -1\n",
    "dabdetr_yolo_like_anchor_update = False\n",
    "dabdetr_deformable_encoder = False\n",
    "dabdetr_deformable_decoder = False\n",
    "use_deformable_box_attn = False\n",
    "box_attn_type = 'roi_align'\n",
    "dec_layer_number = None\n",
    "num_feature_levels = 5              # Key change: 5 scales\n",
    "enc_n_points = 4\n",
    "dec_n_points = 4\n",
    "decoder_layer_noise = False\n",
    "dln_xy_noise = 0.2\n",
    "dln_hw_noise = 0.2\n",
    "add_channel_attention = False\n",
    "add_pos_value = False\n",
    "two_stage_type = 'standard'\n",
    "two_stage_pat_embed = 0\n",
    "two_stage_add_query_num = 0\n",
    "two_stage_bbox_embed_share = False\n",
    "two_stage_class_embed_share = False\n",
    "two_stage_learn_wh = False\n",
    "two_stage_default_hw = 0.05\n",
    "two_stage_keep_all_tokens = False\n",
    "num_select = 300\n",
    "transformer_activation = 'relu'\n",
    "batch_norm_type = 'FrozenBatchNorm2d'\n",
    "masks = False\n",
    "aux_loss = True\n",
    "set_cost_class = 2.0\n",
    "set_cost_bbox = 5.0\n",
    "set_cost_giou = 2.0\n",
    "cls_loss_coef = 1.0\n",
    "mask_loss_coef = 1.0\n",
    "dice_loss_coef = 1.0\n",
    "bbox_loss_coef = 5.0\n",
    "giou_loss_coef = 2.0\n",
    "enc_loss_coef = 1.0\n",
    "interm_loss_coef = 1.0\n",
    "no_interm_box_loss = False\n",
    "focal_alpha = 0.5                   # Improved for minority classes\n",
    "\n",
    "decoder_sa_type = 'sa'\n",
    "matcher_type = 'HungarianMatcher'\n",
    "decoder_module_seq = ['sa', 'ca', 'ffn']\n",
    "nms_iou_threshold = -1\n",
    "\n",
    "dec_pred_bbox_embed_share = True\n",
    "dec_pred_class_embed_share = True\n",
    "\n",
    "# for dn\n",
    "use_dn = True\n",
    "dn_number = 100\n",
    "dn_box_noise_scale = 0.4\n",
    "dn_label_noise_ratio = 0.5\n",
    "embed_init_tgt = True\n",
    "\n",
    "match_unstable_error = True\n",
    "\n",
    "# for ema\n",
    "use_ema = False\n",
    "ema_decay = 0.9997\n",
    "ema_epoch = 0\n",
    "\n",
    "use_detached_boxes_dec_out = False\n",
    "\n",
    "data = dict(\n",
    "    train=dict(\n",
    "        ann_file='/content/coco_dataset/annotations/instances_train2017.json',\n",
    "        img_prefix='/content/coco_dataset/train2017/',\n",
    "    ),\n",
    "    val=dict(\n",
    "        ann_file='/content/coco_dataset/annotations/instances_val2017.json',\n",
    "        img_prefix='/content/coco_dataset/val2017/',\n",
    "    ),\n",
    ")\n",
    "\n",
    "optimizer = dict(\n",
    "    type='AdamW',\n",
    "    lr=1e-4,               # Slightly reduced for smaller batch\n",
    "    weight_decay=1e-4,\n",
    "    eps=1e-8,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "optimizer_config = dict(\n",
    "    grad_clip=dict(max_norm=0.1, norm_type=2),\n",
    "    accumulate_grad_batches=4      # Increased to compensate for smaller batch\n",
    ")\n",
    "\n",
    "lr_config = dict(\n",
    "    policy='step',\n",
    "    step=[8, 11],\n",
    "    gamma=0.1,\n",
    "    warmup='linear',\n",
    "    warmup_iters=1000,\n",
    "    warmup_ratio=0.0001\n",
    ")\n",
    "\n",
    "runner = dict(type='EpochBasedRunner', max_epochs=12)\n",
    "checkpoint_config = dict(interval=2, max_keep_ckpts=3)\n",
    "evaluation = dict(interval=2, metric='bbox', save_best='auto')\n",
    "\n",
    "fp16 = dict(loss_scale='dynamic')\n",
    "\"\"\"\n",
    "\n",
    "with open(\"configs/dino_swinl_ultra.py\", \"w\") as f:\n",
    "    f.write(config_content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c29NB0_aDU3o"
   },
   "source": [
    "# Swin 4 scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCRefkgqATl5"
   },
   "outputs": [],
   "source": [
    "# model = dict(\n",
    "#     num_classes=18,\n",
    "#     dn_labelbook_size=19,\n",
    "#     num_feature_levels=5,\n",
    "#     # Increase model capacity\n",
    "#     # hidden_dim=512,         # Increase from default 256\n",
    "#     # nheads=16,             # Increase attention heads\n",
    "#     # num_queries=1200,      # More queries for better detection\n",
    "# )\n",
    "\n",
    "import os\n",
    "\n",
    "config_content = \"\"\"data_aug_scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "data_aug_max_size = 1333\n",
    "data_aug_scales2_resize = [400, 500, 600]\n",
    "data_aug_scales2_crop = [384, 600]\n",
    "\n",
    "data_aug_scale_overlap = None\n",
    "\n",
    "num_classes=18\n",
    "dn_labelbook_size=19\n",
    "\n",
    "lr = 0.0001\n",
    "param_dict_type = 'default'\n",
    "lr_backbone = 1e-05\n",
    "lr_backbone_names = ['backbone.0']\n",
    "lr_linear_proj_names = ['reference_points', 'sampling_offsets']\n",
    "lr_linear_proj_mult = 0.1\n",
    "ddetr_lr_param = False\n",
    "batch_size = 8\n",
    "weight_decay = 0.0001\n",
    "epochs = 12\n",
    "lr_drop = 11\n",
    "save_checkpoint_interval = 1\n",
    "clip_max_norm = 0.1\n",
    "onecyclelr = False\n",
    "multi_step_lr = False\n",
    "lr_drop_list = [33, 45]\n",
    "\n",
    "\n",
    "modelname = 'dino'\n",
    "frozen_weights = None\n",
    "backbone = 'swin_L_384_22k'\n",
    "use_checkpoint = True\n",
    "\n",
    "dilation = False\n",
    "position_embedding = 'sine'\n",
    "pe_temperatureH = 20\n",
    "pe_temperatureW = 20\n",
    "return_interm_indices = [1, 2, 3]\n",
    "backbone_freeze_keywords = None\n",
    "enc_layers = 6\n",
    "dec_layers = 6\n",
    "unic_layers = 0\n",
    "pre_norm = False\n",
    "dim_feedforward = 2048\n",
    "hidden_dim = 256\n",
    "dropout = 0.0\n",
    "nheads = 8\n",
    "num_queries = 900\n",
    "query_dim = 4\n",
    "num_patterns = 0\n",
    "pdetr3_bbox_embed_diff_each_layer = False\n",
    "pdetr3_refHW = -1\n",
    "random_refpoints_xy = False\n",
    "fix_refpoints_hw = -1\n",
    "dabdetr_yolo_like_anchor_update = False\n",
    "dabdetr_deformable_encoder = False\n",
    "dabdetr_deformable_decoder = False\n",
    "use_deformable_box_attn = False\n",
    "box_attn_type = 'roi_align'\n",
    "dec_layer_number = None\n",
    "num_feature_levels = 4\n",
    "enc_n_points = 4\n",
    "dec_n_points = 4\n",
    "decoder_layer_noise = False\n",
    "dln_xy_noise = 0.2\n",
    "dln_hw_noise = 0.2\n",
    "add_channel_attention = False\n",
    "add_pos_value = False\n",
    "two_stage_type = 'standard'\n",
    "two_stage_pat_embed = 0\n",
    "two_stage_add_query_num = 0\n",
    "two_stage_bbox_embed_share = False\n",
    "two_stage_class_embed_share = False\n",
    "two_stage_learn_wh = False\n",
    "two_stage_default_hw = 0.05\n",
    "two_stage_keep_all_tokens = False\n",
    "num_select = 300\n",
    "transformer_activation = 'relu'\n",
    "batch_norm_type = 'FrozenBatchNorm2d'\n",
    "masks = False\n",
    "aux_loss = True\n",
    "set_cost_class = 2.0\n",
    "set_cost_bbox = 5.0\n",
    "set_cost_giou = 2.0\n",
    "cls_loss_coef = 1.0\n",
    "mask_loss_coef = 1.0\n",
    "dice_loss_coef = 1.0\n",
    "bbox_loss_coef = 5.0\n",
    "giou_loss_coef = 2.0\n",
    "enc_loss_coef = 1.0\n",
    "interm_loss_coef = 1.0\n",
    "no_interm_box_loss = False\n",
    "focal_alpha = 0.25\n",
    "\n",
    "decoder_sa_type = 'sa' # ['sa', 'ca_label', 'ca_content']\n",
    "matcher_type = 'HungarianMatcher' # or SimpleMinsumMatcher\n",
    "decoder_module_seq = ['sa', 'ca', 'ffn']\n",
    "nms_iou_threshold = -1\n",
    "\n",
    "dec_pred_bbox_embed_share = True\n",
    "dec_pred_class_embed_share = True\n",
    "\n",
    "# for dn\n",
    "use_dn = True\n",
    "dn_number = 100\n",
    "dn_box_noise_scale = 0.4\n",
    "dn_label_noise_ratio = 0.5\n",
    "embed_init_tgt = True\n",
    "\n",
    "match_unstable_error = True\n",
    "\n",
    "# for ema\n",
    "use_ema = False\n",
    "ema_decay = 0.9997\n",
    "ema_epoch = 0\n",
    "\n",
    "use_detached_boxes_dec_out = False\n",
    "\n",
    "data = dict(\n",
    "    train=dict(\n",
    "        ann_file='/content/coco_dataset/annotations/instances_train2017.json',\n",
    "        img_prefix='/content/coco_dataset/train2017/',\n",
    "    ),\n",
    "    val=dict(\n",
    "        ann_file='/content/coco_dataset/annotations/instances_val2017.json',\n",
    "        img_prefix='/content/coco_dataset/val2017/',\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Scale learning rate with massive batch size (32/4 = 8x original)\n",
    "optimizer = dict(\n",
    "    type='AdamW',\n",
    "    lr=2e-4,               # Scale up significantly\n",
    "    weight_decay=1e-4,\n",
    "    eps=1e-8,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "optimizer_config = dict(\n",
    "    grad_clip=dict(max_norm=0.1, norm_type=2),\n",
    "    accumulate_grad_batches=2\n",
    ")\n",
    "\n",
    "lr_config = dict(\n",
    "    policy='step',\n",
    "    step=[8, 11],         # Adjust schedule for faster convergence\n",
    "    gamma=0.1,\n",
    "    warmup='linear',\n",
    "    warmup_iters=1000,     # More warmup for high LR\n",
    "    warmup_ratio=0.0001\n",
    ")\n",
    "\n",
    "runner = dict(type='EpochBasedRunner', max_epochs=12)\n",
    "checkpoint_config = dict(interval=2, max_keep_ckpts=3)\n",
    "evaluation = dict(interval=2, metric='bbox', save_best='auto')\n",
    "\n",
    "# Mixed precision for efficiency\n",
    "fp16 = dict(loss_scale='dynamic')\n",
    "\"\"\"\n",
    "\n",
    "with open(\"configs/dino_swinl_ultra.py\", \"w\") as f:\n",
    "    f.write(config_content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8EiiMPmHtYN",
    "outputId": "483567b9-1e9e-4863-ea22-d7ab6b4e0b73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: yapf==0.40.1 in /usr/local/lib/python3.11/dist-packages (0.40.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.11/dist-packages (from yapf==0.40.1) (8.7.0)\n",
      "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf==0.40.1) (4.3.8)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from yapf==0.40.1) (2.2.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.6.0->yapf==0.40.1) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install yapf==0.40.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hiwgbJQlEVsy",
    "outputId": "09feed24-4a15-4b79-c1ad-4ab457b5ee2b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "def inspect_checkpoint_layers(checkpoint_path):\n",
    "    \"\"\"\n",
    "    Loads a PyTorch checkpoint and prints the keys (layer names) and their shapes\n",
    "    from its state_dict.\n",
    "    Handles common checkpoint structures.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "        model_state_dict = None\n",
    "\n",
    "        if 'model' in checkpoint:\n",
    "            print(\"Found 'model' key in checkpoint.\")\n",
    "            model_state_dict = checkpoint['model']\n",
    "        elif 'state_dict' in checkpoint: # Another common key for model weights\n",
    "            print(\"Found 'state_dict' key in checkpoint.\")\n",
    "            model_state_dict = checkpoint['state_dict']\n",
    "        elif isinstance(checkpoint, OrderedDict) or isinstance(checkpoint, dict):\n",
    "             # Check if the checkpoint itself is a state_dict\n",
    "             is_state_dict = True\n",
    "             if not checkpoint: # Empty dict\n",
    "                 is_state_dict = False\n",
    "             else:\n",
    "                 for k, v in checkpoint.items():\n",
    "                     if not isinstance(k, str) or not isinstance(v, torch.Tensor):\n",
    "                         is_state_dict = False\n",
    "                         break\n",
    "             if is_state_dict:\n",
    "                print(\"Checkpoint appears to be a raw state_dict itself.\")\n",
    "                model_state_dict = checkpoint\n",
    "             else:\n",
    "                print(\"Could not automatically determine the model state_dict.\")\n",
    "                print(\"Available top-level keys in checkpoint:\", list(checkpoint.keys()) if isinstance(checkpoint, dict) else \"Checkpoint is not a dict.\")\n",
    "                return\n",
    "        else:\n",
    "            print(\"Checkpoint format not recognized or does not contain model weights in expected keys ('model', 'state_dict').\")\n",
    "            print(\"Checkpoint type:\", type(checkpoint))\n",
    "            return\n",
    "\n",
    "        if model_state_dict:\n",
    "            print(f\"\\nLayers and shapes found in the model's state_dict:\")\n",
    "            max_key_len = 0\n",
    "            if model_state_dict.keys(): # Ensure not empty\n",
    "                max_key_len = max(len(key) for key in model_state_dict.keys())\n",
    "\n",
    "            for key, value in model_state_dict.items():\n",
    "                print(f\"{key:<{max_key_len}} : {list(value.shape)}\")\n",
    "            print(f\"\\nTotal number of parameter tensors in state_dict: {len(model_state_dict.keys())}\")\n",
    "        else:\n",
    "            print(\"No model state_dict could be extracted from the checkpoint.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or inspecting checkpoint: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- IMPORTANT: SET THE PATH TO YOUR CHECKPOINT ---\n",
    "    checkpoint_file = '/content/checkpoint0029_5scale_swin.pth'\n",
    "    # Example: checkpoint_file = 'path/to/your/checkpoint.pth'\n",
    "\n",
    "    if checkpoint_file:\n",
    "        inspect_checkpoint_layers(checkpoint_file)\n",
    "    else:\n",
    "        print(\"Please set the 'checkpoint_file' variable in the script to your checkpoint's path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "an0onyTBE7lK",
    "outputId": "3bafd3f0-5b7b-4359-85d2-74891bba2a80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config\t\t\t\t   main_2.py\t     run_with_submitit.py\n",
      "datasets\t\t\t   main.py\t     scripts\n",
      "engine.py\t\t\t   models\t     tools\n",
      "figs\t\t\t\t   __pycache__\t     util\n",
      "inference_and_visualization.ipynb  README.md\n",
      "LICENSE\t\t\t\t   requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2022 IDEA. All Rights Reserved.\n",
    "# ------------------------------------------------------------------------\n",
    "main_2 = \"\"\"import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from util.get_param_dicts import get_param_dict\n",
    "from util.logger import setup_logger\n",
    "from util.slconfig import DictAction, SLConfig\n",
    "from util.utils import ModelEma, BestMetricHolder\n",
    "import util.misc as utils\n",
    "import datasets\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "from engine import evaluate, train_one_epoch, test\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
    "    parser.add_argument('--config_file', '-c', type=str, required=True)\n",
    "    parser.add_argument('--options',\n",
    "                        nargs='+',\n",
    "                        action=DictAction,\n",
    "                        help='override some settings in the used config, the key-value pair '\n",
    "                             'in xxx=yyy format will be merged into config file.')\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', type=str, default='/comp_robot/cv_public_dataset/COCO2017/')\n",
    "    parser.add_argument('--coco_panoptic_path', type=str)\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "    parser.add_argument('--fix_size', action='store_true')\n",
    "\n",
    "    # training parameters\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--note', default='',\n",
    "                        help='add some notes to the experiment')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--pretrain_model_path', help='load from other checkpoint')\n",
    "    parser.add_argument('--finetune_ignore', type=str, nargs='+')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    parser.add_argument('--test', action='store_true')\n",
    "    parser.add_argument('--debug', action='store_true')\n",
    "    parser.add_argument('--find_unused_params', action='store_true')\n",
    "    parser.add_argument('--save_results', action='store_true')\n",
    "    parser.add_argument('--save_log', action='store_true')\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "    parser.add_argument('--rank', default=0, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument(\"--local_rank\", type=int, help='local rank for DistributedDataParallel')\n",
    "    parser.add_argument('--amp', action='store_true',\n",
    "                        help=\"Train with mixed precision\")\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def build_model_main(args):\n",
    "    # we use register to maintain models from catdet6 on.\n",
    "    from models.registry import MODULE_BUILD_FUNCS\n",
    "    assert args.modelname in MODULE_BUILD_FUNCS._module_dict, f\"Model {args.modelname} not found in registry.\"\n",
    "    build_func = MODULE_BUILD_FUNCS.get(args.modelname)\n",
    "    model, criterion, postprocessors = build_func(args)\n",
    "    return model, criterion, postprocessors\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    utils.init_distributed_mode(args)\n",
    "    # load cfg file and update the args\n",
    "    print(\"Loading config file from {}\".format(args.config_file))\n",
    "    time.sleep(args.rank * 0.02)\n",
    "    cfg = SLConfig.fromfile(args.config_file)\n",
    "    if args.options is not None:\n",
    "        cfg.merge_from_dict(args.options)\n",
    "\n",
    "    if args.rank == 0:\n",
    "        save_cfg_path = os.path.join(args.output_dir, \"config_cfg.py\")\n",
    "        cfg.dump(save_cfg_path)\n",
    "        save_json_path = os.path.join(args.output_dir, \"config_args_raw.json\")\n",
    "        with open(save_json_path, 'w') as f:\n",
    "            json.dump(vars(args), f, indent=2)\n",
    "\n",
    "    cfg_dict = cfg._cfg_dict.to_dict()\n",
    "    args_vars = vars(args)\n",
    "    for k, v in cfg_dict.items():\n",
    "        if k not in args_vars:\n",
    "            setattr(args, k, v)\n",
    "        else:\n",
    "            # Allow overriding if the key is already in args from command line\n",
    "            # This behavior might be intended if options are meant to override direct args too\n",
    "            # Original code raised ValueError, but often overriding is desired.\n",
    "            # For safety, let's log if an arg is being overridden by config.\n",
    "            if getattr(args, k) != v and args.rank == 0: # Check if different and log only on main process\n",
    "                 print(f\"INFO: Argument '{k}' from command line ({getattr(args, k)}) is being kept, value from config file ({v}) is ignored.\")\n",
    "            # If strict \"args only\" is needed, uncomment the ValueError:\n",
    "            # raise ValueError(\"Key {} can used by args only\".format(k))\n",
    "            pass # Default behavior now is command-line args take precedence if already set.\n",
    "\n",
    "    # update some new args temporally\n",
    "    if not getattr(args, 'use_ema', None):\n",
    "        args.use_ema = False\n",
    "    if not getattr(args, 'debug', None):\n",
    "        args.debug = False\n",
    "\n",
    "    # setup logger\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    logger = setup_logger(output=os.path.join(args.output_dir, 'info.txt'), distributed_rank=args.rank, color=False,\n",
    "                          name=\"DINO\") # Changed name for clarity\n",
    "    logger.info(\"git:\\\\n  {}\\\\n\".format(utils.get_sha()))\n",
    "    logger.info(\"Command: \" + ' '.join(sys.argv))\n",
    "    if args.rank == 0:\n",
    "        save_json_path = os.path.join(args.output_dir, \"config_args_all.json\")\n",
    "        with open(save_json_path, 'w') as f:\n",
    "            json.dump(vars(args), f, indent=2)\n",
    "        logger.info(\"Full config saved to {}\".format(save_json_path))\n",
    "\n",
    "    logger.info('world size: {}'.format(args.world_size))\n",
    "    logger.info('rank: {}'.format(args.rank))\n",
    "    logger.info('local_rank: {}'.format(args.local_rank))\n",
    "    logger.info(\"args: \" + str(args) + '\\\\n')\n",
    "\n",
    "    if args.frozen_weights is not None:\n",
    "        assert args.masks, \"Frozen training is meant for segmentation only\"\n",
    "\n",
    "    print(args) # This is already printing args, good for quick check\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = args.seed + utils.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # --- DEBUG PRINTS FOR MODEL BUILDING ---\n",
    "    logger.info(f\"DEBUG main.py: Args before build_model_main: num_classes={args.num_classes}, \"\n",
    "                f\"dn_labelbook_size={args.dn_labelbook_size}, use_dn={getattr(args, 'use_dn', 'Not SetInArgs')}, \"\n",
    "                f\"dn_number={getattr(args, 'dn_number', 'Not SetInArgs')}\")\n",
    "    # --- END DEBUG PRINTS ---\n",
    "\n",
    "    model, criterion, postprocessors = build_model_main(args)\n",
    "    wo_class_error = False # What is this? Defaulting to False seems standard.\n",
    "    model.to(device)\n",
    "\n",
    "    # ema\n",
    "    if args.use_ema:\n",
    "        ema_m = ModelEma(model, args.ema_decay)\n",
    "    else:\n",
    "        ema_m = None\n",
    "\n",
    "    model_without_ddp = model\n",
    "    if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu],\n",
    "                                                          find_unused_parameters=args.find_unused_params)\n",
    "        model_without_ddp = model.module\n",
    "\n",
    "    # --- DEBUG PRINTS FOR MODEL AND CRITERION ---\n",
    "    logger.info(\"DEBUG main.py: Model and Criterion built.\")\n",
    "    if hasattr(criterion, 'num_classes'):\n",
    "        logger.info(f\"DEBUG main.py: Criterion num_classes: {criterion.num_classes}\")\n",
    "    else:\n",
    "        logger.info(\"DEBUG main.py: Criterion does not have a direct 'num_classes' attribute to inspect here.\")\n",
    "\n",
    "    # Inspecting model's class-specific layers\n",
    "    m_to_inspect = model_without_ddp\n",
    "    if hasattr(m_to_inspect, 'class_embed') and m_to_inspect.class_embed is not None:\n",
    "        if hasattr(m_to_inspect.class_embed, 'weight'): # Single class_embed layer\n",
    "             logger.info(f\"DEBUG main.py: Model class_embed weight shape: {m_to_inspect.class_embed.weight.shape}\")\n",
    "        elif isinstance(m_to_inspect.class_embed, torch.nn.ModuleList) and len(m_to_inspect.class_embed) > 0: # DINO uses ModuleList\n",
    "            if hasattr(m_to_inspect.class_embed[-1], 'out_proj') and hasattr(m_to_inspect.class_embed[-1].out_proj, 'weight'): # Common DINO structure\n",
    "                 logger.info(f\"DEBUG main.py: Model final class_embed layer (out_proj) weight shape: {m_to_inspect.class_embed[-1].out_proj.weight.shape}\")\n",
    "            elif hasattr(m_to_inspect.class_embed[-1], 'weight'):\n",
    "                 logger.info(f\"DEBUG main.py: Model final class_embed layer weight shape: {m_to_inspect.class_embed[-1].weight.shape}\")\n",
    "            else:\n",
    "                 logger.info(f\"DEBUG main.py: Model final class_embed layer structure not immediately parsable for weight shape.\")\n",
    "        else:\n",
    "            logger.info(f\"DEBUG main.py: Model class_embed found but structure not immediately parsable for weight shape.\")\n",
    "\n",
    "    if hasattr(m_to_inspect, 'label_enc') and m_to_inspect.label_enc is not None and hasattr(m_to_inspect.label_enc, 'weight'): # For DN label encoder\n",
    "        logger.info(f\"DEBUG main.py: Model label_enc weight shape: {m_to_inspect.label_enc.weight.shape}\")\n",
    "    else:\n",
    "        logger.info(f\"DEBUG main.py: Model label_enc (for DN) not found or has no weight attribute.\")\n",
    "    # --- END DEBUG PRINTS ---\n",
    "\n",
    "\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info('number of params:' + str(n_parameters))\n",
    "    # logger.info(\"params:\\\\n\"+json.dumps({n: p.numel() for n, p in model.named_parameters() if p.requires_grad}, indent=2)) # Can be very verbose\n",
    "\n",
    "    param_dicts = get_param_dict(args, model_without_ddp)\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                                  weight_decay=args.weight_decay)\n",
    "\n",
    "    dataset_train = build_dataset(image_set='train', args=args)\n",
    "    dataset_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "    if args.distributed:\n",
    "        sampler_train = DistributedSampler(dataset_train)\n",
    "        sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
    "    else:\n",
    "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "        sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                   collate_fn=utils.collate_fn, num_workers=args.num_workers, pin_memory=True) # Added pin_memory\n",
    "    data_loader_val = DataLoader(dataset_val, 1, sampler=sampler_val, # Val batch_size often 1\n",
    "                                 drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers, pin_memory=True) # Added pin_memory\n",
    "\n",
    "\n",
    "    if args.onecyclelr:\n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=args.lr, steps_per_epoch=len(data_loader_train), epochs=args.epochs, pct_start=0.2)\n",
    "    elif args.multi_step_lr:\n",
    "        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.lr_drop_list)\n",
    "    else: # Default step LR\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop if hasattr(args, 'lr_drop') else args.epochs // 2 ) # Safer default for lr_drop\n",
    "        # Ensure lr_drop is less than total epochs\n",
    "        if hasattr(args, 'lr_drop') and args.lr_drop >= args.epochs and args.rank == 0:\n",
    "            logger.warning(f\"lr_drop ({args.lr_drop}) is >= total epochs ({args.epochs}). LR will not drop with StepLR.\")\n",
    "\n",
    "\n",
    "    if args.dataset_file == \"coco_panoptic\":\n",
    "        # We also evaluate AP during panoptic training, on original coco DS\n",
    "        coco_val = datasets.coco.build(\"val\", args)\n",
    "        base_ds = get_coco_api_from_dataset(coco_val)\n",
    "    else:\n",
    "        base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "    if args.frozen_weights is not None:\n",
    "        checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "        model_without_ddp.detr.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "    output_dir = Path(args.output_dir)\n",
    "    if os.path.exists(os.path.join(args.output_dir, 'checkpoint.pth')): # Resume from output_dir's checkpoint if it exists\n",
    "        args.resume = os.path.join(args.output_dir, 'checkpoint.pth')\n",
    "        logger.info(f\"Found checkpoint.pth in output_dir, setting resume to: {args.resume}\")\n",
    "\n",
    "\n",
    "    if args.resume:\n",
    "        if args.resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "        if args.use_ema:\n",
    "            if 'ema_model' in checkpoint:\n",
    "                logger.info(\"Loading EMA model from resume checkpoint\")\n",
    "                ema_m.module.load_state_dict(utils.clean_state_dict(checkpoint['ema_model']))\n",
    "            else:\n",
    "                logger.warning(\"No EMA model in resume checkpoint, reinitializing EMA.\")\n",
    "                del ema_m\n",
    "                ema_m = ModelEma(model, args.ema_decay) # Reinitialize if not in checkpoint\n",
    "\n",
    "        if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "            args.start_epoch = checkpoint['epoch'] + 1\n",
    "            logger.info(f\"Resuming training from epoch {args.start_epoch}\")\n",
    "\n",
    "\n",
    "    if (not args.resume or args.eval) and args.pretrain_model_path: # Load pretrain if not resuming for training or if evaluating\n",
    "        logger.info(f\"Loading pretrain model from {args.pretrain_model_path}\")\n",
    "        checkpoint = torch.load(args.pretrain_model_path, map_location='cpu')\n",
    "\n",
    "        # Handle cases where checkpoint might be just the model state_dict or a dict containing 'model'\n",
    "        if 'model' in checkpoint:\n",
    "            model_state_dict = checkpoint['model']\n",
    "        elif 'state_dict' in checkpoint: # Another common key\n",
    "            model_state_dict = checkpoint['state_dict']\n",
    "        else: # Assume the checkpoint itself is the state_dict\n",
    "            model_state_dict = checkpoint\n",
    "            logger.info(\"Pretrain checkpoint does not have 'model' or 'state_dict' key, assuming it's the state_dict itself.\")\n",
    "\n",
    "        from collections import OrderedDict\n",
    "        _ignorekeywordlist = args.finetune_ignore if args.finetune_ignore else []\n",
    "        ignorelist = [] # Stores actually ignored keys\n",
    "\n",
    "        def check_keep(keyname, ignorekeywordlist):\n",
    "            for keyword in ignorekeywordlist:\n",
    "                if keyword in keyname:\n",
    "                    ignorelist.append(keyname) # Log the key that was ignored\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        logger.info(f\"Attempting to load weights, ignoring keywords: {_ignorekeywordlist}\")\n",
    "\n",
    "        # Clean state dict (e.g., remove 'module.' prefix)\n",
    "        cleaned_state_dict = utils.clean_state_dict(model_state_dict)\n",
    "\n",
    "        # Filter state dict\n",
    "        _tmp_st = OrderedDict({k: v for k, v in cleaned_state_dict.items() if check_keep(k, _ignorekeywordlist)})\n",
    "\n",
    "        #############################################################################################################\n",
    "        if 'transformer.tgt_embed.weight' in _tmp_st:\n",
    "            old_embed = _tmp_st['transformer.tgt_embed.weight']\n",
    "\n",
    "            # Try multiple ways to get num_queries dynamically\n",
    "            current_num_queries = None\n",
    "\n",
    "            # Method 1: From global variable (if available in main.py)\n",
    "            if 'num_queries' in globals():\n",
    "                current_num_queries = num_queries\n",
    "                logger.info(f\"num_queries set to global variable: {current_num_queries}\")\n",
    "\n",
    "            # Method 2: From model's current state\n",
    "            elif hasattr(model_without_ddp, 'transformer') and hasattr(model_without_ddp.transformer, 'tgt_embed'):\n",
    "                current_num_queries = model_without_ddp.transformer.tgt_embed.weight.shape[0]\n",
    "                logger.info(f\"num_queries set to global variable: {current_num_queries}\")\n",
    "\n",
    "            # Method 3: From args\n",
    "            elif hasattr(args, 'num_queries'):\n",
    "                current_num_queries = args.num_queries\n",
    "                logger.info(f\"num_queries set to global variable: {current_num_queries}\")\n",
    "\n",
    "            # Fallback: Use old size (no change)\n",
    "            else:\n",
    "                current_num_queries = old_embed.shape[0]\n",
    "                logger.info(\"Warning: Could not determine num_queries, using checkpoint size\")\n",
    "                logger.info(f\"num_queries set to global variable: {current_num_queries}\")\n",
    "\n",
    "            embed_dim = old_embed.shape[1]\n",
    "            new_embed = torch.zeros(current_num_queries, embed_dim)\n",
    "\n",
    "            # Keep original embeddings and initialize new ones\n",
    "            min_queries = min(old_embed.shape[0], current_num_queries)\n",
    "            new_embed[:min_queries] = old_embed[:min_queries]\n",
    "\n",
    "            if current_num_queries > old_embed.shape[0]:\n",
    "                additional_queries = current_num_queries - old_embed.shape[0]\n",
    "                new_embed[old_embed.shape[0]:] = torch.randn(additional_queries, embed_dim) * 0.02\n",
    "                logger.info(f\"Dynamically resized: Preserved {old_embed.shape[0]} embeddings, initialized {additional_queries} new ones\")\n",
    "\n",
    "            _tmp_st['transformer.tgt_embed.weight'] = new_embed\n",
    "        #############################################################################################################\n",
    "\n",
    "        logger.info(\"Keys ignored due to finetune_ignore: {}\".format(json.dumps(sorted(list(set(ignorelist))), indent=2)))\n",
    "\n",
    "        # Log missing keys in the model that are not in the filtered checkpoint\n",
    "        model_keys = set(model_without_ddp.state_dict().keys())\n",
    "        checkpoint_keys = set(_tmp_st.keys())\n",
    "        missing_in_checkpoint = list(model_keys - checkpoint_keys)\n",
    "        unexpected_in_checkpoint = list(checkpoint_keys - model_keys)\n",
    "\n",
    "        logger.info(f\"Keys in model but not in (filtered) checkpoint (will be randomly initialized if not ignored): {json.dumps(sorted(missing_in_checkpoint), indent=2)}\")\n",
    "        logger.info(f\"Keys in (filtered) checkpoint but not in model (will be ignored by load_state_dict): {json.dumps(sorted(unexpected_in_checkpoint), indent=2)}\")\n",
    "\n",
    "        _load_output = model_without_ddp.load_state_dict(_tmp_st, strict=False)\n",
    "        logger.info(f\"load_state_dict output (missing_keys, unexpected_keys): {str(_load_output)}\")\n",
    "\n",
    "        # For EMA, if pretraining and EMA is used, should ideally also load EMA weights if available,\n",
    "        # but typically pretrain_model_path refers to non-EMA weights.\n",
    "        # Re-initializing EMA here is safer if not resuming EMA state.\n",
    "        if args.use_ema and not args.resume : # If not resuming an EMA state\n",
    "            logger.info(\"Re-initializing EMA after loading pretrain_model_path as EMA state was not loaded from pretrain.\")\n",
    "            if ema_m is not None:\n",
    "                del ema_m\n",
    "            ema_m = ModelEma(model, args.ema_decay)\n",
    "\n",
    "\n",
    "    if args.eval:\n",
    "        os.environ['EVAL_FLAG'] = 'TRUE'\n",
    "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
    "                                              data_loader_val, base_ds, device, args.output_dir,\n",
    "                                              wo_class_error=wo_class_error, args=args, logger=logger) # Pass logger\n",
    "        if args.output_dir and coco_evaluator is not None and hasattr(coco_evaluator, 'coco_eval') and \"bbox\" in coco_evaluator.coco_eval:\n",
    "            utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "\n",
    "        log_stats = {**{f'test_{k}': v for k, v in test_stats.items()}}\n",
    "        if args.output_dir and utils.is_main_process():\n",
    "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\\\n\")\n",
    "        return\n",
    "\n",
    "    print(\"Start training\")\n",
    "    start_time = time.time()\n",
    "    best_map_holder = BestMetricHolder(use_ema=args.use_ema)\n",
    "\n",
    "    # --- DEBUG NOTE ---\n",
    "    logger.info(\"DEBUG main.py: For detailed DN loss debugging (why they might be zero), \"\n",
    "                \"consider adding print statements inside 'engine.py:train_one_epoch' to inspect: \"\n",
    "                \"1. The 'targets' list/dict for DN components before passing to criterion. \"\n",
    "                \"2. The 'outputs' from the model, especially parts relevant to DN. \"\n",
    "                \"3. The individual DN loss values calculated inside the criterion's forward method.\")\n",
    "    # --- END DEBUG NOTE ---\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        if args.distributed:\n",
    "            sampler_train.set_epoch(epoch)\n",
    "\n",
    "        train_stats = train_one_epoch(\n",
    "            model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "            args.clip_max_norm, wo_class_error=wo_class_error, lr_scheduler=lr_scheduler, args=args,\n",
    "            logger=(logger if args.save_log else None), ema_m=ema_m)\n",
    "\n",
    "        if args.output_dir:\n",
    "            checkpoint_paths = [output_dir / 'checkpoint.pth'] # Always save the latest\n",
    "            # extra checkpoint before LR drop and every N epochs\n",
    "            if hasattr(args, 'lr_drop') and (epoch + 1) % args.lr_drop == 0 :\n",
    "                 checkpoint_paths.append(output_dir / f'checkpoint_lr_drop_epoch{epoch:04}.pth')\n",
    "            if hasattr(args, 'save_checkpoint_interval') and (epoch + 1) % args.save_checkpoint_interval == 0:\n",
    "                checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "\n",
    "            # De-duplicate paths if multiple conditions met\n",
    "            checkpoint_paths = sorted(list(set(checkpoint_paths)))\n",
    "\n",
    "            for checkpoint_path in checkpoint_paths:\n",
    "                weights = {\n",
    "                    'model': model_without_ddp.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'args': args, # Save args for reproducibility\n",
    "                }\n",
    "                if args.use_ema:\n",
    "                    weights.update({\n",
    "                        'ema_model': ema_m.module.state_dict(),\n",
    "                    })\n",
    "                utils.save_on_master(weights, checkpoint_path)\n",
    "\n",
    "        if not args.onecyclelr: # StepLR and MultiStepLR are stepped per epoch\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        test_stats, coco_evaluator = evaluate(\n",
    "            model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir,\n",
    "            wo_class_error=wo_class_error, args=args, logger=(logger if args.save_log else None)\n",
    "        )\n",
    "\n",
    "        map_regular = test_stats['coco_eval_bbox'][0] if 'coco_eval_bbox' in test_stats and test_stats['coco_eval_bbox'] else 0.0\n",
    "        _isbest = best_map_holder.update(map_regular, epoch, is_ema=False)\n",
    "        if _isbest:\n",
    "            checkpoint_path = output_dir / 'checkpoint_best_regular.pth'\n",
    "            utils.save_on_master({\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "                # also save optimizer and scheduler?\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "\n",
    "        log_stats = {\n",
    "            **{f'train_{k}': v for k, v in train_stats.items()},\n",
    "            **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "        }\n",
    "\n",
    "        if args.use_ema:\n",
    "            ema_test_stats, ema_coco_evaluator = evaluate(\n",
    "                ema_m.module, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir,\n",
    "                wo_class_error=wo_class_error, args=args, logger=(logger if args.save_log else None)\n",
    "            )\n",
    "            log_stats.update({f'ema_test_{k}': v for k, v in ema_test_stats.items()})\n",
    "            map_ema = ema_test_stats['coco_eval_bbox'][0] if 'coco_eval_bbox' in ema_test_stats and ema_test_stats['coco_eval_bbox'] else 0.0\n",
    "            _isbest_ema = best_map_holder.update(map_ema, epoch, is_ema=True)\n",
    "            if _isbest_ema:\n",
    "                checkpoint_path = output_dir / 'checkpoint_best_ema.pth'\n",
    "                utils.save_on_master({\n",
    "                    'model': ema_m.module.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'args': args,\n",
    "                }, checkpoint_path)\n",
    "\n",
    "        log_stats.update(best_map_holder.summary())\n",
    "\n",
    "        ep_paras = {\n",
    "            'epoch': epoch,\n",
    "            'n_parameters': n_parameters\n",
    "        }\n",
    "        log_stats.update(ep_paras)\n",
    "        try:\n",
    "            log_stats.update({'now_time': str(datetime.datetime.now())})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        epoch_time_str = str(datetime.timedelta(seconds=int(epoch_time)))\n",
    "        log_stats['epoch_time'] = epoch_time_str\n",
    "\n",
    "        if args.output_dir and utils.is_main_process():\n",
    "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\\\n\")\n",
    "\n",
    "            if coco_evaluator is not None:\n",
    "                eval_dir = output_dir / 'eval'\n",
    "                eval_dir.mkdir(exist_ok=True)\n",
    "                if \"bbox\" in coco_evaluator.coco_eval:\n",
    "                    filenames = [eval_dir / 'latest.pth']\n",
    "                    if epoch % 50 == 0: # Save historical eval results less frequently\n",
    "                        filenames.append(eval_dir / f'{epoch:03}.pth')\n",
    "                    for name in filenames:\n",
    "                        torch.save(coco_evaluator.coco_eval[\"bbox\"].eval, name)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))\n",
    "\n",
    "    # remove the copied files.\n",
    "    copyfilelist = vars(args).get('copyfilelist')\n",
    "    if copyfilelist and args.local_rank == 0 :\n",
    "        from datasets.data_util import remove_list_of_files # Assuming a function that can remove a list\n",
    "        print(f\"Removing copied files: {copyfilelist}\")\n",
    "        remove_list_of_files(copyfilelist) # Adapt if your util is different\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser('DINO training and evaluation script', parents=[get_args_parser()]) # Updated description\n",
    "    args = parser.parse_args()\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    main(args)\n",
    "\"\"\"\n",
    "\n",
    "with open(\"/content/DINO/main_2.py\", \"w\") as f:\n",
    "    f.write(main_2.strip())\n",
    "\n",
    "!ls /content/DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iwz8R-qSG6a-"
   },
   "outputs": [],
   "source": [
    "# To fix this, you need to edit the file /usr/local/lib/python3.11/dist-packages/pycocotools/cocoeval.py at line 378 and\n",
    "# change np.float to float or np.float64 as suggested by the error message.\n",
    "\n",
    "new_f = \"\"\"__author__ = 'tsungyi'\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from . import mask as maskUtils\n",
    "import copy\n",
    "\n",
    "class COCOeval:\n",
    "    # Interface for evaluating detection on the Microsoft COCO dataset.\n",
    "    #\n",
    "    # The usage for CocoEval is as follows:\n",
    "    #  cocoGt=..., cocoDt=...       # load dataset and results\n",
    "    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n",
    "    #  E.params.recThrs = ...;      # set parameters as desired\n",
    "    #  E.evaluate();                # run per image evaluation\n",
    "    #  E.accumulate();              # accumulate per image results\n",
    "    #  E.summarize();               # display summary metrics of results\n",
    "    # For example usage see evalDemo.m and http://mscoco.org/.\n",
    "    #\n",
    "    # The evaluation parameters are as follows (defaults in brackets):\n",
    "    #  imgIds     - [all] N img ids to use for evaluation\n",
    "    #  catIds     - [all] K cat ids to use for evaluation\n",
    "    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n",
    "    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n",
    "    #  areaRng    - [...] A=4 object area ranges for evaluation\n",
    "    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n",
    "    #  iouType    - ['segm'] set iouType to 'segm', 'bbox' or 'keypoints'\n",
    "    #  iouType replaced the now DEPRECATED useSegm parameter.\n",
    "    #  useCats    - [1] if true use category labels for evaluation\n",
    "    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n",
    "    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n",
    "    #\n",
    "    # evaluate(): evaluates detections on every image and every category and\n",
    "    # concats the results into the \"evalImgs\" with fields:\n",
    "    #  dtIds      - [1xD] id for each of the D detections (dt)\n",
    "    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n",
    "    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n",
    "    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n",
    "    #  dtScores   - [1xD] confidence of each dt\n",
    "    #  gtIgnore   - [1xG] ignore flag for each gt\n",
    "    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n",
    "    #\n",
    "    # accumulate(): accumulates the per-image, per-category evaluation\n",
    "    # results in \"evalImgs\" into the dictionary \"eval\" with fields:\n",
    "    #  params     - parameters used for evaluation\n",
    "    #  date       - date evaluation was performed\n",
    "    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n",
    "    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n",
    "    #  recall     - [TxKxAxM] max recall for every evaluation setting\n",
    "    # Note: precision and recall==-1 for settings with no gt objects.\n",
    "    #\n",
    "    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n",
    "    #\n",
    "    # Microsoft COCO Toolbox.      version 2.0\n",
    "    # Data, paper, and tutorials available at:  http://mscoco.org/\n",
    "    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n",
    "    # Licensed under the Simplified BSD License [see coco/license.txt]\n",
    "    def __init__(self, cocoGt=None, cocoDt=None, iouType='segm'):\n",
    "        '''\n",
    "        Initialize CocoEval using coco APIs for gt and dt\n",
    "        :param cocoGt: coco object with ground truth annotations\n",
    "        :param cocoDt: coco object with detection results\n",
    "        :return: None\n",
    "        '''\n",
    "        if not iouType:\n",
    "            print('iouType not specified. use default iouType segm')\n",
    "        self.cocoGt   = cocoGt              # ground truth COCO API\n",
    "        self.cocoDt   = cocoDt              # detections COCO API\n",
    "        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n",
    "        self.eval     = {}                  # accumulated evaluation results\n",
    "        self._gts = defaultdict(list)       # gt for evaluation\n",
    "        self._dts = defaultdict(list)       # dt for evaluation\n",
    "        self.params = Params(iouType=iouType) # parameters\n",
    "        self._paramsEval = {}               # parameters for evaluation\n",
    "        self.stats = []                     # result summarization\n",
    "        self.ious = {}                      # ious between all gts and dts\n",
    "        if not cocoGt is None:\n",
    "            self.params.imgIds = sorted(cocoGt.getImgIds())\n",
    "            self.params.catIds = sorted(cocoGt.getCatIds())\n",
    "\n",
    "\n",
    "    def _prepare(self):\n",
    "        '''\n",
    "        Prepare ._gts and ._dts for evaluation based on params\n",
    "        :return: None\n",
    "        '''\n",
    "        def _toMask(anns, coco):\n",
    "            # modify ann['segmentation'] by reference\n",
    "            for ann in anns:\n",
    "                rle = coco.annToRLE(ann)\n",
    "                ann['segmentation'] = rle\n",
    "        p = self.params\n",
    "        if p.useCats:\n",
    "            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n",
    "            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n",
    "        else:\n",
    "            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n",
    "            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n",
    "\n",
    "        # convert ground truth to mask if iouType == 'segm'\n",
    "        if p.iouType == 'segm':\n",
    "            _toMask(gts, self.cocoGt)\n",
    "            _toMask(dts, self.cocoDt)\n",
    "        # set ignore flag\n",
    "        for gt in gts:\n",
    "            gt['ignore'] = gt['ignore'] if 'ignore' in gt else 0\n",
    "            gt['ignore'] = 'iscrowd' in gt and gt['iscrowd']\n",
    "            if p.iouType == 'keypoints':\n",
    "                gt['ignore'] = (gt['num_keypoints'] == 0) or gt['ignore']\n",
    "        self._gts = defaultdict(list)       # gt for evaluation\n",
    "        self._dts = defaultdict(list)       # dt for evaluation\n",
    "        for gt in gts:\n",
    "            self._gts[gt['image_id'], gt['category_id']].append(gt)\n",
    "        for dt in dts:\n",
    "            self._dts[dt['image_id'], dt['category_id']].append(dt)\n",
    "        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n",
    "        self.eval     = {}                  # accumulated evaluation results\n",
    "\n",
    "    def evaluate(self):\n",
    "        '''\n",
    "        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n",
    "        :return: None\n",
    "        '''\n",
    "        tic = time.time()\n",
    "        print('Running per image evaluation...')\n",
    "        p = self.params\n",
    "        # add backward compatibility if useSegm is specified in params\n",
    "        if not p.useSegm is None:\n",
    "            p.iouType = 'segm' if p.useSegm == 1 else 'bbox'\n",
    "            print('useSegm (deprecated) is not None. Running {} evaluation'.format(p.iouType))\n",
    "        print('Evaluate annotation type *{}*'.format(p.iouType))\n",
    "        p.imgIds = list(np.unique(p.imgIds))\n",
    "        if p.useCats:\n",
    "            p.catIds = list(np.unique(p.catIds))\n",
    "        p.maxDets = sorted(p.maxDets)\n",
    "        self.params=p\n",
    "\n",
    "        self._prepare()\n",
    "        # loop through images, area range, max detection number\n",
    "        catIds = p.catIds if p.useCats else [-1]\n",
    "\n",
    "        if p.iouType == 'segm' or p.iouType == 'bbox':\n",
    "            computeIoU = self.computeIoU\n",
    "        elif p.iouType == 'keypoints':\n",
    "            computeIoU = self.computeOks\n",
    "        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n",
    "                        for imgId in p.imgIds\n",
    "                        for catId in catIds}\n",
    "\n",
    "        evaluateImg = self.evaluateImg\n",
    "        maxDet = p.maxDets[-1]\n",
    "        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n",
    "                 for catId in catIds\n",
    "                 for areaRng in p.areaRng\n",
    "                 for imgId in p.imgIds\n",
    "             ]\n",
    "        self._paramsEval = copy.deepcopy(self.params)\n",
    "        toc = time.time()\n",
    "        print('DONE (t={:0.2f}s).'.format(toc-tic))\n",
    "\n",
    "    def computeIoU(self, imgId, catId):\n",
    "        p = self.params\n",
    "        if p.useCats:\n",
    "            gt = self._gts[imgId,catId]\n",
    "            dt = self._dts[imgId,catId]\n",
    "        else:\n",
    "            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n",
    "            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n",
    "        if len(gt) == 0 and len(dt) ==0:\n",
    "            return []\n",
    "        inds = np.argsort([-d['score'] for d in dt], kind='mergesort')\n",
    "        dt = [dt[i] for i in inds]\n",
    "        if len(dt) > p.maxDets[-1]:\n",
    "            dt=dt[0:p.maxDets[-1]]\n",
    "\n",
    "        if p.iouType == 'segm':\n",
    "            g = [g['segmentation'] for g in gt]\n",
    "            d = [d['segmentation'] for d in dt]\n",
    "        elif p.iouType == 'bbox':\n",
    "            g = [g['bbox'] for g in gt]\n",
    "            d = [d['bbox'] for d in dt]\n",
    "        else:\n",
    "            raise Exception('unknown iouType for iou computation')\n",
    "\n",
    "        # compute iou between each dt and gt region\n",
    "        iscrowd = [int(o['iscrowd']) for o in gt]\n",
    "        ious = maskUtils.iou(d,g,iscrowd)\n",
    "        return ious\n",
    "\n",
    "    def computeOks(self, imgId, catId):\n",
    "        p = self.params\n",
    "        # dimention here should be Nxm\n",
    "        gts = self._gts[imgId, catId]\n",
    "        dts = self._dts[imgId, catId]\n",
    "        inds = np.argsort([-d['score'] for d in dts], kind='mergesort')\n",
    "        dts = [dts[i] for i in inds]\n",
    "        if len(dts) > p.maxDets[-1]:\n",
    "            dts = dts[0:p.maxDets[-1]]\n",
    "        # if len(gts) == 0 and len(dts) == 0:\n",
    "        if len(gts) == 0 or len(dts) == 0:\n",
    "            return []\n",
    "        ious = np.zeros((len(dts), len(gts)))\n",
    "        sigmas = p.kpt_oks_sigmas\n",
    "        vars = (sigmas * 2)**2\n",
    "        k = len(sigmas)\n",
    "        # compute oks between each detection and ground truth object\n",
    "        for j, gt in enumerate(gts):\n",
    "            # create bounds for ignore regions(double the gt bbox)\n",
    "            g = np.array(gt['keypoints'])\n",
    "            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n",
    "            k1 = np.count_nonzero(vg > 0)\n",
    "            bb = gt['bbox']\n",
    "            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n",
    "            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n",
    "            for i, dt in enumerate(dts):\n",
    "                d = np.array(dt['keypoints'])\n",
    "                xd = d[0::3]; yd = d[1::3]\n",
    "                if k1>0:\n",
    "                    # measure the per-keypoint distance if keypoints visible\n",
    "                    dx = xd - xg\n",
    "                    dy = yd - yg\n",
    "                else:\n",
    "                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n",
    "                    z = np.zeros((k))\n",
    "                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n",
    "                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n",
    "                e = (dx**2 + dy**2) / vars / (gt['area']+np.spacing(1)) / 2\n",
    "                if k1 > 0:\n",
    "                    e=e[vg > 0]\n",
    "                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n",
    "        return ious\n",
    "\n",
    "    def evaluateImg(self, imgId, catId, aRng, maxDet):\n",
    "        '''\n",
    "        perform evaluation for single category and image\n",
    "        :return: dict (single image results)\n",
    "        '''\n",
    "        p = self.params\n",
    "        if p.useCats:\n",
    "            gt = self._gts[imgId,catId]\n",
    "            dt = self._dts[imgId,catId]\n",
    "        else:\n",
    "            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n",
    "            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n",
    "        if len(gt) == 0 and len(dt) ==0:\n",
    "            return None\n",
    "\n",
    "        for g in gt:\n",
    "            if g['ignore'] or (g['area']<aRng[0] or g['area']>aRng[1]):\n",
    "                g['_ignore'] = 1\n",
    "            else:\n",
    "                g['_ignore'] = 0\n",
    "\n",
    "        # sort dt highest score first, sort gt ignore last\n",
    "        gtind = np.argsort([g['_ignore'] for g in gt], kind='mergesort')\n",
    "        gt = [gt[i] for i in gtind]\n",
    "        dtind = np.argsort([-d['score'] for d in dt], kind='mergesort')\n",
    "        dt = [dt[i] for i in dtind[0:maxDet]]\n",
    "        iscrowd = [int(o['iscrowd']) for o in gt]\n",
    "        # load computed ious\n",
    "        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n",
    "\n",
    "        T = len(p.iouThrs)\n",
    "        G = len(gt)\n",
    "        D = len(dt)\n",
    "        gtm  = np.zeros((T,G))\n",
    "        dtm  = np.zeros((T,D))\n",
    "        gtIg = np.array([g['_ignore'] for g in gt])\n",
    "        dtIg = np.zeros((T,D))\n",
    "        if not len(ious)==0:\n",
    "            for tind, t in enumerate(p.iouThrs):\n",
    "                for dind, d in enumerate(dt):\n",
    "                    # information about best match so far (m=-1 -> unmatched)\n",
    "                    iou = min([t,1-1e-10])\n",
    "                    m   = -1\n",
    "                    for gind, g in enumerate(gt):\n",
    "                        # if this gt already matched, and not a crowd, continue\n",
    "                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n",
    "                            continue\n",
    "                        # if dt matched to reg gt, and on ignore gt, stop\n",
    "                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n",
    "                            break\n",
    "                        # continue to next gt unless better match made\n",
    "                        if ious[dind,gind] < iou:\n",
    "                            continue\n",
    "                        # if match successful and best so far, store appropriately\n",
    "                        iou=ious[dind,gind]\n",
    "                        m=gind\n",
    "                    # if match made store id of match for both dt and gt\n",
    "                    if m ==-1:\n",
    "                        continue\n",
    "                    dtIg[tind,dind] = gtIg[m]\n",
    "                    dtm[tind,dind]  = gt[m]['id']\n",
    "                    gtm[tind,m]     = d['id']\n",
    "        # set unmatched detections outside of area range to ignore\n",
    "        a = np.array([d['area']<aRng[0] or d['area']>aRng[1] for d in dt]).reshape((1, len(dt)))\n",
    "        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n",
    "        # store results for given image and category\n",
    "        return {\n",
    "                'image_id':     imgId,\n",
    "                'category_id':  catId,\n",
    "                'aRng':         aRng,\n",
    "                'maxDet':       maxDet,\n",
    "                'dtIds':        [d['id'] for d in dt],\n",
    "                'gtIds':        [g['id'] for g in gt],\n",
    "                'dtMatches':    dtm,\n",
    "                'gtMatches':    gtm,\n",
    "                'dtScores':     [d['score'] for d in dt],\n",
    "                'gtIgnore':     gtIg,\n",
    "                'dtIgnore':     dtIg,\n",
    "            }\n",
    "\n",
    "    def accumulate(self, p = None):\n",
    "        '''\n",
    "        Accumulate per image evaluation results and store the result in self.eval\n",
    "        :param p: input params for evaluation\n",
    "        :return: None\n",
    "        '''\n",
    "        print('Accumulating evaluation results...')\n",
    "        tic = time.time()\n",
    "        if not self.evalImgs:\n",
    "            print('Please run evaluate() first')\n",
    "        # allows input customized parameters\n",
    "        if p is None:\n",
    "            p = self.params\n",
    "        p.catIds = p.catIds if p.useCats == 1 else [-1]\n",
    "        T           = len(p.iouThrs)\n",
    "        R           = len(p.recThrs)\n",
    "        K           = len(p.catIds) if p.useCats else 1\n",
    "        A           = len(p.areaRng)\n",
    "        M           = len(p.maxDets)\n",
    "        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n",
    "        recall      = -np.ones((T,K,A,M))\n",
    "        scores      = -np.ones((T,R,K,A,M))\n",
    "\n",
    "        # create dictionary for future indexing\n",
    "        _pe = self._paramsEval\n",
    "        catIds = _pe.catIds if _pe.useCats else [-1]\n",
    "        setK = set(catIds)\n",
    "        setA = set(map(tuple, _pe.areaRng))\n",
    "        setM = set(_pe.maxDets)\n",
    "        setI = set(_pe.imgIds)\n",
    "        # get inds to evaluate\n",
    "        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n",
    "        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n",
    "        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n",
    "        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n",
    "        I0 = len(_pe.imgIds)\n",
    "        A0 = len(_pe.areaRng)\n",
    "        # retrieve E at each category, area range, and max number of detections\n",
    "        for k, k0 in enumerate(k_list):\n",
    "            Nk = k0*A0*I0\n",
    "            for a, a0 in enumerate(a_list):\n",
    "                Na = a0*I0\n",
    "                for m, maxDet in enumerate(m_list):\n",
    "                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n",
    "                    E = [e for e in E if not e is None]\n",
    "                    if len(E) == 0:\n",
    "                        continue\n",
    "                    dtScores = np.concatenate([e['dtScores'][0:maxDet] for e in E])\n",
    "\n",
    "                    # different sorting method generates slightly different results.\n",
    "                    # mergesort is used to be consistent as Matlab implementation.\n",
    "                    inds = np.argsort(-dtScores, kind='mergesort')\n",
    "                    dtScoresSorted = dtScores[inds]\n",
    "\n",
    "                    dtm  = np.concatenate([e['dtMatches'][:,0:maxDet] for e in E], axis=1)[:,inds]\n",
    "                    dtIg = np.concatenate([e['dtIgnore'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n",
    "                    gtIg = np.concatenate([e['gtIgnore'] for e in E])\n",
    "                    npig = np.count_nonzero(gtIg==0 )\n",
    "                    if npig == 0:\n",
    "                        continue\n",
    "                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n",
    "                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n",
    "\n",
    "                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float64)\n",
    "                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float64)\n",
    "                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n",
    "                        tp = np.array(tp)\n",
    "                        fp = np.array(fp)\n",
    "                        nd = len(tp)\n",
    "                        rc = tp / npig\n",
    "                        pr = tp / (fp+tp+np.spacing(1))\n",
    "                        q  = np.zeros((R,))\n",
    "                        ss = np.zeros((R,))\n",
    "\n",
    "                        if nd:\n",
    "                            recall[t,k,a,m] = rc[-1]\n",
    "                        else:\n",
    "                            recall[t,k,a,m] = 0\n",
    "\n",
    "                        # numpy is slow without cython optimization for accessing elements\n",
    "                        # use python array gets significant speed improvement\n",
    "                        pr = pr.tolist(); q = q.tolist()\n",
    "\n",
    "                        for i in range(nd-1, 0, -1):\n",
    "                            if pr[i] > pr[i-1]:\n",
    "                                pr[i-1] = pr[i]\n",
    "\n",
    "                        inds = np.searchsorted(rc, p.recThrs, side='left')\n",
    "                        try:\n",
    "                            for ri, pi in enumerate(inds):\n",
    "                                q[ri] = pr[pi]\n",
    "                                ss[ri] = dtScoresSorted[pi]\n",
    "                        except:\n",
    "                            pass\n",
    "                        precision[t,:,k,a,m] = np.array(q)\n",
    "                        scores[t,:,k,a,m] = np.array(ss)\n",
    "        self.eval = {\n",
    "            'params': p,\n",
    "            'counts': [T, R, K, A, M],\n",
    "            'date': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'precision': precision,\n",
    "            'recall':   recall,\n",
    "            'scores': scores,\n",
    "        }\n",
    "        toc = time.time()\n",
    "        print('DONE (t={:0.2f}s).'.format( toc-tic))\n",
    "\n",
    "    def summarize(self):\n",
    "        '''\n",
    "        Compute and display summary metrics for evaluation results.\n",
    "        Note this functin can *only* be applied on the default parameter setting\n",
    "        '''\n",
    "        def _summarize( ap=1, iouThr=None, areaRng='all', maxDets=100 ):\n",
    "            p = self.params\n",
    "            iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'\n",
    "            titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n",
    "            typeStr = '(AP)' if ap==1 else '(AR)'\n",
    "            iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n",
    "                if iouThr is None else '{:0.2f}'.format(iouThr)\n",
    "\n",
    "            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n",
    "            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n",
    "            if ap == 1:\n",
    "                # dimension of precision: [TxRxKxAxM]\n",
    "                s = self.eval['precision']\n",
    "                # IoU\n",
    "                if iouThr is not None:\n",
    "                    t = np.where(iouThr == p.iouThrs)[0]\n",
    "                    s = s[t]\n",
    "                s = s[:,:,:,aind,mind]\n",
    "            else:\n",
    "                # dimension of recall: [TxKxAxM]\n",
    "                s = self.eval['recall']\n",
    "                if iouThr is not None:\n",
    "                    t = np.where(iouThr == p.iouThrs)[0]\n",
    "                    s = s[t]\n",
    "                s = s[:,:,aind,mind]\n",
    "            if len(s[s>-1])==0:\n",
    "                mean_s = -1\n",
    "            else:\n",
    "                mean_s = np.mean(s[s>-1])\n",
    "            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n",
    "            return mean_s\n",
    "        def _summarizeDets():\n",
    "            stats = np.zeros((12,))\n",
    "            stats[0] = _summarize(1)\n",
    "            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n",
    "            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n",
    "            stats[3] = _summarize(1, areaRng='small', maxDets=self.params.maxDets[2])\n",
    "            stats[4] = _summarize(1, areaRng='medium', maxDets=self.params.maxDets[2])\n",
    "            stats[5] = _summarize(1, areaRng='large', maxDets=self.params.maxDets[2])\n",
    "            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n",
    "            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n",
    "            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n",
    "            stats[9] = _summarize(0, areaRng='small', maxDets=self.params.maxDets[2])\n",
    "            stats[10] = _summarize(0, areaRng='medium', maxDets=self.params.maxDets[2])\n",
    "            stats[11] = _summarize(0, areaRng='large', maxDets=self.params.maxDets[2])\n",
    "            return stats\n",
    "        def _summarizeKps():\n",
    "            stats = np.zeros((10,))\n",
    "            stats[0] = _summarize(1, maxDets=20)\n",
    "            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n",
    "            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n",
    "            stats[3] = _summarize(1, maxDets=20, areaRng='medium')\n",
    "            stats[4] = _summarize(1, maxDets=20, areaRng='large')\n",
    "            stats[5] = _summarize(0, maxDets=20)\n",
    "            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n",
    "            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n",
    "            stats[8] = _summarize(0, maxDets=20, areaRng='medium')\n",
    "            stats[9] = _summarize(0, maxDets=20, areaRng='large')\n",
    "            return stats\n",
    "        if not self.eval:\n",
    "            raise Exception('Please run accumulate() first')\n",
    "        iouType = self.params.iouType\n",
    "        if iouType == 'segm' or iouType == 'bbox':\n",
    "            summarize = _summarizeDets\n",
    "        elif iouType == 'keypoints':\n",
    "            summarize = _summarizeKps\n",
    "        self.stats = summarize()\n",
    "\n",
    "    def __str__(self):\n",
    "        self.summarize()\n",
    "\n",
    "class Params:\n",
    "    '''\n",
    "    Params for coco evaluation api\n",
    "    '''\n",
    "    def setDetParams(self):\n",
    "        self.imgIds = []\n",
    "        self.catIds = []\n",
    "        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n",
    "        self.iouThrs = np.linspace(.5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)\n",
    "        self.recThrs = np.linspace(.0, 1.00, int(np.round((1.00 - .0) / .01)) + 1, endpoint=True)\n",
    "        self.maxDets = [1, 10, 100]\n",
    "        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n",
    "        self.areaRngLbl = ['all', 'small', 'medium', 'large']\n",
    "        self.useCats = 1\n",
    "\n",
    "    def setKpParams(self):\n",
    "        self.imgIds = []\n",
    "        self.catIds = []\n",
    "        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n",
    "        self.iouThrs = np.linspace(.5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)\n",
    "        self.recThrs = np.linspace(.0, 1.00, int(np.round((1.00 - .0) / .01)) + 1, endpoint=True)\n",
    "        self.maxDets = [20]\n",
    "        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n",
    "        self.areaRngLbl = ['all', 'medium', 'large']\n",
    "        self.useCats = 1\n",
    "        self.kpt_oks_sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n",
    "\n",
    "    def __init__(self, iouType='segm'):\n",
    "        if iouType == 'segm' or iouType == 'bbox':\n",
    "            self.setDetParams()\n",
    "        elif iouType == 'keypoints':\n",
    "            self.setKpParams()\n",
    "        else:\n",
    "            raise Exception('iouType not supported')\n",
    "        self.iouType = iouType\n",
    "        # useSegm is deprecated\n",
    "        self.useSegm = None\n",
    "\"\"\"\n",
    "with open(\"/usr/local/lib/python3.11/dist-packages/pycocotools/cocoeval.py\", \"w\") as f:\n",
    "    f.write(new_f.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8X6X7XNWMeAP",
    "outputId": "4a604053-e50f-447b-bf28-95e7ecc415e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'gpu_monitor.py': No such file or directory\n",
      "rm: cannot remove 'nohup.out': No such file or directory\n",
      "rm: cannot remove 'training.log': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -rf dino_swinl_ft/\n",
    "!rm -rf dino_swinl_ultra/\n",
    "!rm gpu_monitor.py\n",
    "!rm nohup.out\n",
    "!rm training.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJgSZnj-AyKv",
    "outputId": "100bbea0-46e2-4134-cfd7-ff6dcc85f7a8"
   },
   "outputs": [],
   "source": [
    "!python -u /content/DINO/main_2.py \\\n",
    "  --config_file /content/configs/dino_swinl_ultra.py \\\n",
    "  --coco_path /content/coco_dataset \\\n",
    "  --pretrain_model_path /content/checkpoint0029_5scale_swin.pth \\\n",
    "  --finetune_ignore label_enc.weight class_embed \\\n",
    "  --output_dir /content/dino_swinl_ultra \\\n",
    "  --num_workers 10 \\\n",
    "  --amp \\\n",
    "  --seed 42 \\\n",
    "  --options batch_size=2 data.workers_per_gpu=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laCWAA0OUWJr",
    "outputId": "77392d98-2243-4d86-e92d-ddb0ad578fd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw------- 1 root root 437950 May 24 02:56 /content/coco_dataset/train2017/13641.jpg\n",
      "-rw------- 1 root root 590885 May 24 02:56 /content/coco_dataset/train2017/13642.jpg\n",
      "16000\n",
      "annotations  train2017\tval2017\n"
     ]
    }
   ],
   "source": [
    "# Check if the specific failing files exist\n",
    "!ls -la /content/coco_dataset/train2017/13641.jpg\n",
    "!ls -la /content/coco_dataset/train2017/13642.jpg\n",
    "\n",
    "# Check total file count\n",
    "!ls /content/coco_dataset/train2017/ | wc -l\n",
    "!ls /content/coco_dataset/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4x1NL3fV-mn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "002cf07e6a84494aa7c1999fbc64309c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "04a88b242f954f97a0b9d52ee8a11a49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93d6ebf54dea460fa2b473010a5161b1",
      "max": 20000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_002cf07e6a84494aa7c1999fbc64309c",
      "value": 20000
     }
    },
    "4e97eb37c2444e67b77c7cabb680f8de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87a81102cf2a43ee99ec093af83d0560": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e97eb37c2444e67b77c7cabb680f8de",
      "placeholder": "​",
      "style": "IPY_MODEL_ef04f49b5eff470a86b96c814ede3208",
      "value": "Copying images to train/val splits: 100%"
     }
    },
    "93d6ebf54dea460fa2b473010a5161b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3c5c88ac00347b994f066e40d56c54e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4f9d18e168e4e75a69c7b03f12f3ce9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cac14aeac8b54383aea0f73377642f1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4f9d18e168e4e75a69c7b03f12f3ce9",
      "placeholder": "​",
      "style": "IPY_MODEL_de20d30f83b24b67845eb6581b3208d1",
      "value": " 20000/20000 [03:45&lt;00:00, 473.12files/s]"
     }
    },
    "d74b69b23a7249b69cd0add89f458f6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_87a81102cf2a43ee99ec093af83d0560",
       "IPY_MODEL_04a88b242f954f97a0b9d52ee8a11a49",
       "IPY_MODEL_cac14aeac8b54383aea0f73377642f1b"
      ],
      "layout": "IPY_MODEL_a3c5c88ac00347b994f066e40d56c54e"
     }
    },
    "de20d30f83b24b67845eb6581b3208d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef04f49b5eff470a86b96c814ede3208": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
